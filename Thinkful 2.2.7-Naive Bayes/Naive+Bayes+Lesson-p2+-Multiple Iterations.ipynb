{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import html5lib\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_con_mat(matrix):\n",
    "    #use confusion matrix to compute accuracy (good calls divided by all calls made)\n",
    "    print('Accuracy of {}%'.format(round(100*\n",
    "                                         (matrix[1][1]+matrix[0][0])/\n",
    "                                         (matrix[1][0]+matrix[0][1]+matrix[1][1]+matrix[0][0])),2))\n",
    "    \n",
    "    print('Sensitivity of {}%'.format(round(100*matrix[1][1]/(matrix[1][1] + matrix[1][0]),2)))\n",
    "    print('Specificity of {}%'.format(round(100*matrix[0][1]/(matrix[0][0] + matrix[0][1]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in a list of 2000+ positive words\n",
    "pos_words_loc = ('https://github.com/gurkpet/Thinkful-Lessons/blob/master/'\n",
    "             'Thinkful%202.2.7-Naive%20Bayes/Word%20Lists/positive-words.txt')\n",
    "pos_words_list = pd.read_html(pos_words_loc, skiprows=35)\n",
    "\n",
    "#clean up the read in by removing the first column of junk data\n",
    "for row in pos_words_list:\n",
    "   del row[0]    \n",
    "\n",
    "#convert list into dataframe\n",
    "pos_words = pd.DataFrame(pos_words_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_pos_words = list(pos_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in amazon file\n",
    "amzn_file_loc = ('https://raw.githubusercontent.com/gurkpet/Thinkful-Lessons/master/'\n",
    "            'Thinkful%202.2.7-Naive%20Bayes/amazon_cells_labelled.txt')\n",
    "\n",
    "amzn_dat = pd.read_csv(amzn_file_loc, delimiter= '\\t', header=None)\n",
    "#rename column headers\n",
    "amzn_dat.columns = ['review', 'positive_review']\n",
    "amzn_target = amzn_dat['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in list_pos_words:\n",
    "    amzn_dat[str(key)] = amzn_dat.review.str.contains(str(key), case = False)\n",
    "amzn_dat['positive_review'] = (amzn_dat['positive_review'] == 1)\n",
    "data_amzn = amzn_dat[list_pos_words]\n",
    "amzn_target = amzn_dat['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 83.0%\n",
      "Sensitivity of 75.8%\n",
      "Specificity of 9.0%\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(data_amzn, amzn_target)\n",
    "pred = bnb.predict(data_amzn)\n",
    "con_mat = confusion_matrix(amzn_target, pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First iteration: 3 folds\n",
    "amzn_fold1 = data_amzn[:333]\n",
    "amzn_fold2 = data_amzn[333:666]\n",
    "amzn_fold3 = data_amzn[666:]\n",
    "\n",
    "#corresponding targets for 3 folds\n",
    "amzn_fold1_targ = amzn_dat['positive_review'][:333]\n",
    "amzn_fold2_targ = amzn_dat['positive_review'][333:666]\n",
    "amzn_fold3_targ = amzn_dat['positive_review'][666:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bnb_fold1 = bnb.fit(amzn_fold1, amzn_fold1_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using fold 1 as the training data and then test folds 2 and 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 60.0%\n",
      "Sensitivity of 27.27%\n",
      "Specificity of 2.55%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold1.predict(amzn_fold1)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ,fold1_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 70.0%\n",
      "Sensitivity of 77.71%\n",
      "Specificity of 37.72%\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold1.predict(amzn_fold2)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ,fold2_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 71.0%\n",
      "Sensitivity of 74.05%\n",
      "Specificity of 32.39%\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold1.predict(amzn_fold3)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ,fold3_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using fold 2 as the training data and then test folds 1 and 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seems there is some severe overfitting, lets continue the cross validation.\n",
    "bnb_fold2 = bnb.fit(amzn_fold2, amzn_fold2_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 63.0%\n",
      "Sensitivity of 30.12%\n",
      "Specificity of 4.19%\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold2.predict(amzn_fold2)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ,fold2_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 72.0%\n",
      "Sensitivity of 54.55%\n",
      "Specificity of 8.28%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold2.predict(amzn_fold1)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ,fold1_pred)\n",
    "\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 74.0%\n",
      "Sensitivity of 49.37%\n",
      "Specificity of 3.98%\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold2.predict(amzn_fold3)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ,fold3_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using fold 3 as the training data and then test folds 1 and 2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Things look even WORSE using fold 2 for training.  Trying fold 3.\n",
    "bnb_fold3 = bnb.fit(amzn_fold3, amzn_fold3_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 69.0%\n",
      "Sensitivity of 35.44%\n",
      "Specificity of 0.0%\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold3.predict(amzn_fold3)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ,fold3_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, to start off, fold 3 trained and tested against itself wasn't very good at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 60.0%\n",
      "Sensitivity of 27.27%\n",
      "Specificity of 2.55%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold3.predict(amzn_fold1)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ,fold1_pred)\n",
    "\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 63.0%\n",
      "Sensitivity of 30.12%\n",
      "Specificity of 4.19%\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold3.predict(amzn_fold2)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ,fold2_pred)\n",
    "\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold three as a training set looks HORRENDOUS.  I think its safe to say that the features we have been using are very poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
