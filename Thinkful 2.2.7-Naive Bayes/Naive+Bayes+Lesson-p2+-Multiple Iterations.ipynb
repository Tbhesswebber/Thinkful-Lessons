{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import html5lib\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_con_mat(matrix):\n",
    "    #use confusion matrix to compute accuracy (good calls divided by all calls made)\n",
    "    print('Accuracy of {}%'.format(round(100*\n",
    "                                         (matrix[1][1]+matrix[0][0])/\n",
    "                                         (matrix[1][0]+matrix[0][1]+matrix[1][1]+matrix[0][0])),2))\n",
    "    \n",
    "\n",
    "    print('Sensitivity of {}%'.format(round(100*matrix[1][1]/(matrix[1][1] + matrix[1][0]),2)))\n",
    "    print('Specificity of {}%'.format(round(100*matrix[0][0]/(matrix[0][0] + matrix[0][1]),2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in a list of 2000+ positive words\n",
    "pos_words_loc = ('https://github.com/gurkpet/Thinkful-Lessons/blob/master/'\n",
    "             'Thinkful%202.2.7-Naive%20Bayes/Word%20Lists/positive-words.txt')\n",
    "pos_words_list = pd.read_html(pos_words_loc, skiprows=35)\n",
    "\n",
    "#clean up the read in by removing the first column of junk data\n",
    "for row in pos_words_list:\n",
    "   del row[0]    \n",
    "\n",
    "#convert list into dataframe\n",
    "pos_words = pd.DataFrame(pos_words_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_pos_words = list(pos_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in amazon file\n",
    "amzn_file_loc = ('https://raw.githubusercontent.com/gurkpet/Thinkful-Lessons/master/'\n",
    "            'Thinkful%202.2.7-Naive%20Bayes/amazon_cells_labelled.txt')\n",
    "\n",
    "df = pd.read_csv(amzn_file_loc, delimiter= '\\t', header=None)\n",
    "#rename column headers\n",
    "amzn_dat = df.copy()\n",
    "amzn_dat.columns = ['review', 'positive_review']\n",
    "amzn_target = amzn_dat['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in list_pos_words:\n",
    "    amzn_dat[str(key)] = amzn_dat.review.str.contains(str(key), case = False)\n",
    "amzn_dat['positive_review'] = (amzn_dat['positive_review'] == 1)\n",
    "data_amzn = amzn_dat[list_pos_words]\n",
    "amzn_target = amzn_dat['positive_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iteration 1 - Train and test entire dataset with positive keywords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 83.0%\n",
      "Sensitivity of 75.8%\n",
      "Specificity of 91.0%\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(data_amzn, amzn_target)\n",
    "pred = bnb.predict(data_amzn)\n",
    "con_mat = confusion_matrix(amzn_target, pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iteration 2- Cross validation of positive keywords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First iteration: 3 folds\n",
    "amzn_fold1 = data_amzn[:333]\n",
    "amzn_fold2 = data_amzn[333:666]\n",
    "amzn_fold3 = data_amzn[666:]\n",
    "\n",
    "#corresponding targets for 3 folds\n",
    "amzn_fold1_targ = amzn_dat['positive_review'][:333]\n",
    "amzn_fold2_targ = amzn_dat['positive_review'][333:666]\n",
    "amzn_fold3_targ = amzn_dat['positive_review'][666:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bnb_fold1 = bnb.fit(amzn_fold1, amzn_fold1_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using fold 1 as the training data and then test folds 2 and 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 85.0%\n",
      "Sensitivity of 92.05%\n",
      "Specificity of 76.43%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold1.predict(amzn_fold1)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ,fold1_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 70.0%\n",
      "Sensitivity of 77.71%\n",
      "Specificity of 62.28%\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold1.predict(amzn_fold2)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ,fold2_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 71.0%\n",
      "Sensitivity of 74.05%\n",
      "Specificity of 67.61%\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold1.predict(amzn_fold3)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ,fold3_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using fold 2 as the training data and then test folds 1 and 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seems there is some severe overfitting, lets continue the cross validation.\n",
    "bnb_fold2 = bnb.fit(amzn_fold2, amzn_fold2_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 79.0%\n",
      "Sensitivity of 63.25%\n",
      "Specificity of 94.61%\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold2.predict(amzn_fold2)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ,fold2_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 72.0%\n",
      "Sensitivity of 54.55%\n",
      "Specificity of 91.72%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold2.predict(amzn_fold1)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ,fold1_pred)\n",
    "\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 74.0%\n",
      "Sensitivity of 49.37%\n",
      "Specificity of 96.02%\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold2.predict(amzn_fold3)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ,fold3_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using fold 3 as the training data and then test folds 1 and 2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Things look even WORSE using fold 2 for training.  Trying fold 3.\n",
    "bnb_fold3 = bnb.fit(amzn_fold3, amzn_fold3_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 69.0%\n",
      "Sensitivity of 35.44%\n",
      "Specificity of 100.0%\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold3.predict(amzn_fold3)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ,fold3_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, to start off, fold 3 trained and tested against itself wasn't very good at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 60.0%\n",
      "Sensitivity of 27.27%\n",
      "Specificity of 97.45%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold3.predict(amzn_fold1)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ,fold1_pred)\n",
    "\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 63.0%\n",
      "Sensitivity of 30.12%\n",
      "Specificity of 95.81%\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold3.predict(amzn_fold2)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ,fold2_pred)\n",
    "\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold three as a training set looks HORRENDOUS.  I think its safe to say that the features we have been using are very poor.  So maybe time to try another method to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Iteration 3- Train and test entire dataset with negative keywords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pulling in a list of negative keywords from someones github\n",
    "neg_words_loc = 'https://raw.githubusercontent.com/williamgunn/SciSentiment/master/negative-words.txt'\n",
    "neg_words_df = pd.read_csv(neg_words_loc, skiprows=35, header = None)\n",
    "neg_words_list = neg_words_df[0]\n",
    "neg_words_list= neg_words_list[neg_words_list!='bull****']\n",
    "neg_words_list= neg_words_list[neg_words_list!='bull----']\n",
    "neg_words_list= neg_words_list[neg_words_list!='f**k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amzn_dat_neg = df.copy()\n",
    "amzn_dat_neg.columns = ['review', 'positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in neg_words_list:\n",
    "    amzn_dat_neg[str(key)] = amzn_dat_neg.review.str.contains(str(key), case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amzn_dat_neg['positive_review'] = (amzn_dat_neg['positive_review'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_amzn_neg = amzn_dat_neg[neg_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amzn_target_neg = amzn_dat_neg['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 76.0%\n",
      "Sensitivity of 97.2%\n",
      "Specificity of 55.2%\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(data_amzn_neg, amzn_target_neg)\n",
    "pred = bnb.predict(data_amzn_neg)\n",
    "con_mat = confusion_matrix(amzn_target_neg, pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems using negative keywords is slightly less effective when training against all the data, but lets doing some cross validation.\n",
    "<h1>Iteration 4- Cross Validation using negative keywords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First iteration: 3 folds\n",
    "amzn_fold1_neg = data_amzn_neg[:333]\n",
    "amzn_fold2_neg = data_amzn_neg[333:666]\n",
    "amzn_fold3_neg = data_amzn_neg[666:]\n",
    "\n",
    "#corresponding targets for 3 folds\n",
    "amzn_fold1_targ_neg = amzn_target_neg[:333]\n",
    "amzn_fold2_targ_neg = amzn_target_neg[333:666]\n",
    "amzn_fold3_targ_neg = amzn_target_neg[666:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb_fold1 = bnb.fit(amzn_fold1_neg, amzn_fold1_targ_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 55.0%\n",
      "Sensitivity of 100.0%\n",
      "Specificity of 5.1%\n",
      "[[  8 149]\n",
      " [  0 176]]\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold1.predict(amzn_fold1_neg)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ_neg,fold1_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 63.0%\n",
      "Sensitivity of 30.12%\n",
      "Specificity of 95.81%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold1.predict(amzn_fold2_neg)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ_neg, fold2_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 69.0%\n",
      "Sensitivity of 35.44%\n",
      "Specificity of 100.0%\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold1.predict(amzn_fold3_neg)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ_neg, fold3_pred)\n",
    "eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb_fold2 = bnb.fit(amzn_fold2_neg, amzn_fold2_targ_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 78.0%\n",
      "Sensitivity of 90.96%\n",
      "Specificity of 64.67%\n",
      "[[108  59]\n",
      " [ 15 151]]\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold2.predict(amzn_fold2_neg)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ_neg,fold2_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 64.0%\n",
      "Sensitivity of 84.09%\n",
      "Specificity of 41.4%\n",
      "[[ 65  92]\n",
      " [ 28 148]]\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold2.predict(amzn_fold1_neg)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ_neg, fold1_pred)\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 62.0%\n",
      "Sensitivity of 81.65%\n",
      "Specificity of 44.89%\n",
      "[[ 79  97]\n",
      " [ 29 129]]\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold2.predict(amzn_fold3_neg)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ_neg,fold3_pred)\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb_fold3 = bnb.fit(amzn_fold3_neg, amzn_fold3_targ_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of the Training Data on itself\n",
      "Accuracy of 54.0%\n",
      "Sensitivity of 2.53%\n",
      "Specificity of 100.0%\n",
      "[[176   0]\n",
      " [154   4]]\n"
     ]
    }
   ],
   "source": [
    "fold3_pred = bnb_fold3.predict(amzn_fold3_neg)\n",
    "con_mat = confusion_matrix(amzn_fold3_targ_neg,fold3_pred)\n",
    "print('This is a test of the Training Data on itself')\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 46.0%\n",
      "Sensitivity of 1.7%\n",
      "Specificity of 96.18%\n",
      "[[151   6]\n",
      " [173   3]]\n"
     ]
    }
   ],
   "source": [
    "fold1_pred = bnb_fold3.predict(amzn_fold1_neg)\n",
    "con_mat = confusion_matrix(amzn_fold1_targ_neg,fold1_pred)\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 51.0%\n",
      "Sensitivity of 3.01%\n",
      "Specificity of 98.8%\n",
      "[[165   2]\n",
      " [161   5]]\n"
     ]
    }
   ],
   "source": [
    "fold2_pred = bnb_fold3.predict(amzn_fold2_neg)\n",
    "con_mat = confusion_matrix(amzn_fold2_targ_neg,fold2_pred)\n",
    "eval_con_mat(con_mat)\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much every iteration seems like an overfit.  They are all pretty innaccurate when cross-validated and always less accurate on the test data than the training data.\n",
    "\n",
    "<h1>I think it would be best to go over these questions with you as I don't really know how to answer.  Especially the which features seem best question</h1>\n",
    "Which seem to perform the best? Why?\n",
    "What features seemed to be most impactful to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
