{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import html5lib\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_con_mat(matrix):\n",
    "    #use confusion matrix to compute accuracy (good calls divided by all calls made)\n",
    "    print('Accuracy of {}%'.format(round(100*\n",
    "                                         (matrix[1][1]+matrix[0][0])/\n",
    "                                         (matrix[1][0]+matrix[0][1]+matrix[1][1]+matrix[0][0])),2))\n",
    "    \n",
    "\n",
    "    print('Sensitivity of {}%'.format(round(100*matrix[1][1]/(matrix[1][1] + matrix[1][0]),2)))\n",
    "    print('Specificity of {}%'.format(round(100*matrix[0][0]/(matrix[0][0] + matrix[0][1]),2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in a list of 2000+ positive words\n",
    "pos_words_loc = ('https://github.com/gurkpet/Thinkful-Lessons/blob/master/'\n",
    "             'Thinkful%202.2.7-Naive%20Bayes/Word%20Lists/positive-words.txt')\n",
    "pos_words_list = pd.read_html(pos_words_loc, skiprows=35)\n",
    "\n",
    "#clean up the read in by removing the first column of junk data\n",
    "for row in pos_words_list:\n",
    "   del row[0]    \n",
    "\n",
    "#convert list into dataframe\n",
    "pos_words = pd.DataFrame(pos_words_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_pos_words = list(pos_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in amazon file\n",
    "amzn_file_loc = ('https://raw.githubusercontent.com/gurkpet/Thinkful-Lessons/master/'\n",
    "            'Thinkful%202.2.7-Naive%20Bayes/amazon_cells_labelled.txt')\n",
    "\n",
    "df = pd.read_csv(amzn_file_loc, delimiter= '\\t', header=None)\n",
    "#rename column headers\n",
    "amzn_dat = df\n",
    "amzn_dat.columns = ['review', 'positive_review']\n",
    "amzn_target = amzn_dat['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in list_pos_words:\n",
    "    amzn_dat[str(key)] = amzn_dat.review.str.contains(str(key), case = False)\n",
    "amzn_dat['positive_review'] = (amzn_dat['positive_review'] == 1)\n",
    "data_amzn = amzn_dat[list_pos_words]\n",
    "amzn_target = amzn_dat['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration no. 1 results\n",
      "Accuracy of 88.0%\n",
      "Sensitivity of 82.18%\n",
      "Specificity of 93.94%\n",
      "\n",
      "Iteration no. 2 results\n",
      "Accuracy of 84.0%\n",
      "Sensitivity of 79.09%\n",
      "Specificity of 88.89%\n",
      "\n",
      "Iteration no. 3 results\n",
      "Accuracy of 80.0%\n",
      "Sensitivity of 73.27%\n",
      "Specificity of 86.87%\n",
      "\n",
      "Iteration no. 4 results\n",
      "Accuracy of 82.0%\n",
      "Sensitivity of 72.16%\n",
      "Specificity of 90.29%\n",
      "\n",
      "Iteration no. 5 results\n",
      "Accuracy of 84.0%\n",
      "Sensitivity of 71.43%\n",
      "Specificity of 94.5%\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "skf = KFold(n_splits=5)\n",
    "x= 0\n",
    "for train, test in skf.split(data_amzn):\n",
    "    x = x+1\n",
    "    fold = data_amzn.ix[train]\n",
    "    keep = data_amzn.ix[test]\n",
    "    targ = amzn_target.ix[train]\n",
    "    bnb = bnb.fit(data_amzn, amzn_target)\n",
    "    pred = bnb.predict(keep)\n",
    "    con_mat = confusion_matrix(amzn_target.ix[test],pred)\n",
    "    print('\\nIteration no. {} results'.format(x))\n",
    "    eval_con_mat(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Is any of the data overfit?:</h5>\n",
    "    Based on the similarities between our five iteriations it doesn't seem that our model is overfitting.\n",
    "\n",
    "<h5>Which seem to perform the best? Why?:</h5>\n",
    "    Iteration number 1 seems have have the best performance.  This is likely due to the skewness between the folds, where the data is more balanced between positive and negative reviews for the Training data for Iteration 1 than the rest of the iterations.  \n",
    "    \n",
    "<h5>What features seemed to be most impactful to performance?:</h5>\n",
    "    ??????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
